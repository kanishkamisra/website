---
abstract: >
  Despite readily memorizing world knowledge about entities, pre-trained language models (LMs) struggle to compose together two or more facts to perform multi-hop reasoning in question-answering tasks. In this work, we propose techniques that improve upon this limitation by relying on random-walks over structured knowledge graphs. Specifically, we use soft-prompts to guide LMs to chain together their encoded knowledge by learning to map multi-hop questions to random-walk paths that lead to the answer. Applying our methods on two T5 LMs shows substantial improvements over standard tuning approaches in answering questions that require multi-hop reasoning.
authors:
- admin 
- Cicero Nogueira dos Santos
- Siamak Shakeri
date: "2023-07-01T00:00:00Z"
# doi: "10.1109/SMC.2019.8914528"
featured: true
links:
- name: ACL Anthology
  url: https://aclanthology.org/2023.findings-acl.62/
publication: In *Findings of the Association for Computational Linguistics 2023*
publication_short: In *Findings of ACL 2023*
publication_types: 
- "1"
#publishDate: "2021-08-01T00:00:00Z"
# slides: example
summary: >
  Using soft-prompts along with random-walks over knowledge graphs to trigger multi-hop reasoning in LMs. Work done during an internship at Google Research.
# tags:
# - Source Themes
title: "Triggering Multi-Hop Reasoning for Question Answering in Language Models using Soft Prompts and Random Walks"
#url_code: 'https://github.com/kanishkamisra/comps'
# url_dataset: '#'
# url_pdf: "papers/fuzzy-risk.pdf"
# url_poster: "posters/cogsci22.pdf"
# url_project: ""
#url_slides: "slides/cogsci2021.pdf"
# url_source: '#'
#url_video: '#'
---

{{% alert note %}}
Click the *Cite* button above to demo the feature to enable visitors to import publication metadata into their reference management software.
{{% /alert %}}


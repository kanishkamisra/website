---
abstract: 'We present $\texttt{minicons}$, an open source library that provides a standard API for researchers interested in conducting behavioral and representational analyses of transformer-based language models (LMs). Specifically, minicons enables researchers to apply analysis methods at two levels: (1) at the prediction level -- by providing functions to efficiently extract word/sentence level probabilities; and (2) at the representational level -- by also facilitating efficient extraction of word/phrase level vectors from one or more layers. In this paper, we describe the library and apply it to two motivating case studies: One focusing on the learning dynamics of the BERT architecture on relative grammatical judgments, and the other on benchmarking 23 different LMs on zero-shot abductive reasoning. minicons is available at [this url](https://github.com/kanishkamisra/minicons).'
authors:
- admin 
date: "2022-03-24T00:00:00Z"
# doi: "10.1109/SMC.2019.8914528"
featured: true
#image:
#  caption: 'Image credit: [**Unsplash**](https://unsplash.com/photos/pLCdAaMFLTE)'
#  focal_point: ""
#  preview_only: false
links:
# - name: NAFIPS
- name: arxiv
  url: https://arxiv.org/abs/2203.13112
publication: In *arxiv Preprint*
publication_short: In *arxiv*
publication_types:
- "3"
publishDate: "2022-03-24T00:00:00Z"
# slides: example
summary: A python library that facilitates behavioral and representational analyses of transformer Language Models.
# tags:
# - Source Themes
title: "minicons: Enabling Flexible Behavioral and Representational Analyses of Transformer Language Models"
url_code: 'https://github.com/kanishkamisra/minicons-experiments'
# url_dataset: '#'
# url_pdf: "papers/nafips21.pdf"
# url_poster: '#'
# url_project: ""
# url_slides: "slides/nafips2020.pdf"
# url_source: '#'
# url_video: '#'
---

{{% alert note %}}
Click the *Cite* button above to cite this paper!
{{% /alert %}}


---
title: 'Introducing $\texttt{minicons}$: Running large scale behavioral analyses on transformer language models'
author: Kanishka Misra
date: '2021-03-21'
slug: minicons-running-large-scale-behavioral-analyses-on-transformer-lms
categories:
  - research
tags:
  - Neural Networks
  - Language Models
  - Natural Language Processing
  - Research
subtitle: ''
summary: 'In this post, I showcase my new python library that implements simple computations to facilitate large-scale evaluation of transformer language models.'
authors: []
lastmod: '2021-03-22T22:52:15-04:00'
featured: no
image:
  caption: 'Made in [draw.io](https://app.diagrams.net/). Icon made by [Freepik](https://www.freepik.com)'
  focal_point: ''
  preview_only: no
projects: []
bibliography: [references.bib]
header-includes:
  - \usepackage{bbm}
  - \usepackage{amsmath}
  - \usepackage{gb4e}
  - \noautomath
---







<div id="premise" class="section level2">
<h2>Premise</h2>
<p>Assume that just like any other day in the Natural Language Processing (NLP) community, you read a tweet about this fancy new transformer <span class="citation">(Vaswani et al. 2017)</span> language model that has recently come out and is touted to solve many NLP tasks!
Let’s call it :robot:.
Now lets say that you are an NLP researcher, and just like me, you are interested in understanding and evaluating models like :robot:.
Fortunately for you, due to the hard work put in by <a href="https://huggingface.co/">Huggingface</a>, :robot: has been made public and now appears on its <a href="https://huggingface.co/models">model hub</a> (for the purposes of this post, let’s assume that :robot: = <a href="https://huggingface.co/distilgpt2"><code>distilgpt2</code></a>).
Assuming for the purposes of this post, you are interested in morpho-syntax,<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a> you might ask the question:</p>
<blockquote>
<p>How well does :robot: learn subject-verb agreement?</p>
</blockquote>
<p>If you were to conduct an experiment to answer this question in the context of native English speakers, you’d present several human subjects pairs of sentences that look like (1) and (2) and ask them to rate which one is more acceptable:</p>
<ol style="list-style-type: decimal">
<li><em>the keys to the cabinet <u><strong>are</strong></u> on the table.</em></li>
<li><em>the keys to the cabinet <u><strong>is</strong></u> on the table.</em></li>
</ol>
<p>In the above examples, knowledge about subject-verb agreement is tested by checking if the agent (human, our model :robot:, etc.) rates (1) as more acceptable than (2), since the number of the verb (<em>is</em> vs. <em>are</em>) has to agree with the subject, <em>keys</em>.
Such an experiment is common to the psycholinguistic domain, where participants are presented with stimuli that are targeted to probe for a particular information.</p>
<p>But how does one perform the same experiment with :robot:?
Since it is a language model, :robot: assigns probabilities to words in a sequence.
Maintaining the assumption that :robot: is the the same as <code>distilgpt2</code>, it estimates the probability of a word <span class="math inline">\(w_i\)</span> in the sequence <span class="math inline">\(s = w_1, w_2, ..., w_{n-1}\)</span> as <span class="math inline">\(p(w_i \mid w_1, ..., w_{i-1})\)</span>.
To calculate the probability of (<em>is</em>/<em>are</em>) in our example sentences, the model computes the expression <span class="math inline">\(p(\textit{is } \text{or} \textit{ are } \mid \textit{ the, keys, to, the, cabinet)}\)</span>.
One could treat this conditional probability as a measure of how plausible the given phrase (<em>the keys to the cabinet is</em> or <em>the keys to the cabinet are</em>) is w.r.t. to the model’s trained parameters, as was done by <span class="citation">Linzen, Dupoux, and Goldberg (2016)</span>, and subsequent works.
Therefore, a good language model of English—just like humans—will tell you that (1) is more plausible than (2), i.e. <span class="math inline">\(p(\textit{are} \mid \textit{the, keys, to, the, cabinet)} &gt; p(\textit{is } \mid \textit{ the, keys, to, the, cabinet)}\)</span>.</p>
</div>
<div id="enter-textttminicons" class="section level2">
<h2>Enter <span class="math inline">\(\texttt{minicons}\)</span>:</h2>
<p><span class="math inline">\(\texttt{minicons}\)</span> is a python library that lets you conduct experiments similar to the one sketched out above on a large scale.
It automates the probability computations of transformer LMs that are accessible through the <code>transformers</code> package by HuggingFace, so that you—the researcher—can instead spend most of your time and effort in carefully designing experiments and stimuli that target aspects of linguistic knowledge that the model should ideally encode.
<span class="math inline">\(\texttt{minicons}\)</span> is on <a href="https://pypi.org/project/minicons/">pypi</a>, and can be installed using the following terminal command:
<!-- > Introducing `minicons`, a python package that lets you run large scale behavior analyses on transformer based LMs. --></p>
<!-- A large part of my PhD research focuses on developing analytical tests to better understand blackbox neural network-based language processing models. -->
<!-- Since many of the tests that have been proposed BLiMP [@warstadt2019blimp]. -->
<pre class="bash"><code>pip install minicons</code></pre>
<p>To score sentences or phrases, you’ll need the <code>scorer</code> module from the library. In the snippet below, I have also loaded other packages that will help demonstrate how one can rapidly run behavioral experiments using <span class="math inline">\(\texttt{minicons}\)</span>.</p>
<pre class="python"><code>from minicons import scorer

import json
import statistics

from torch.utils.data import DataLoader</code></pre>
<p>Sticking to the subject-verb agreement task, one could collect/construct pairs of sentences that differ in their verb/auxiliary verb depending on the subject, such as examples (1) and (2). For simplicity, let’s use the one that appears in a recent dataset/paper called <a href="https://github.com/alexwarstadt/blimp">BLiMP</a> <span class="citation">(Warstadt et al. 2020)</span>.
The BLiMP dataset consists of 67 different stimuli types (1000 pairs of sentences per stimuli-type), each targeted to evaluate a unique linguistic phenomenon based on one of 12 different paradigms (see Table 1 in <span class="citation">Warstadt et al. (2020)</span> for a summary of these paradigms), totaling to 67,000 pairs of sentence stimuli.
One stimuli type relevant to the subject-verb agreement paradigm that you might be interested in is contained in a file called <code>distractor_agreement_relational_noun.jsonl</code>, made publicly available on <a href="https://github.com/alexwarstadt/blimp/blob/master/data/distractor_agreement_relational_noun.jsonl">the lead author’s github</a>.
The stimuli contained in this particular subset only cover one aspect of subject-verb agreement knowledge — one requiring a verb’s agreement with the number of a noun that is held constant across a given pair of sentences (see Table 4 in the same paper to explore other ways of evaluating subject-verb agreement).
Let’s now load the stimuli from the file:</p>
<pre class="python"><code>stimuli = []
with open(&quot;distractor_agreement_relational_noun.jsonl&quot;, &quot;r&quot;) as f:
    for line in f:
        row = json.loads(line)
        stimuli.append([row[&#39;one_prefix_prefix&#39;] + &quot; &quot; + row[&#39;one_prefix_word_good&#39;], row[&#39;one_prefix_prefix&#39;] + &quot; &quot; + row[&#39;one_prefix_word_bad&#39;]])
        
# Let&#39;s take a look at some of the stimuli
for pair in stimuli[:5]:
    print(f&quot;{pair[0]} vs. {pair[1]}&quot;)</code></pre>
<pre><code>## A niece of most senators hasn&#39;t vs. A niece of most senators haven&#39;t
## The sketch of those trucks hasn&#39;t vs. The sketch of those trucks haven&#39;t
## A newspaper article about the Borgias has vs. A newspaper article about the Borgias have
## The niece of most guests has vs. The niece of most guests have
## A sketch of lights doesn&#39;t vs. A sketch of lights don&#39;t</code></pre>
<p>Notice that we only need the left context of the target words (<em>is</em> vs <em>are</em> in the running example), since our model, :robot:, is an incremental LM, i.e., it estimates the probability of a word by conditioning on its left context.
To load our model, <span class="math inline">\(\texttt{minicons.scorer}\)</span> provides the <code>IncrementalLMScorer</code> class to instantiate any Incremental language model that is accessible through huggingface’s <code>transformers</code> library.
For this post, I’ve loaded it onto my cpu but one could also use a <code>cuda</code> device if it is available to them.</p>
<pre class="python"><code>model = scorer.IncrementalLMScorer(&#39;distilgpt2&#39;, &#39;cpu&#39;)</code></pre>
<pre><code>## Using pad_token, but it is not set yet.</code></pre>
<p>Let’s also instantiate a dataloader so that we can run computations in batches as opposed to one at a time.
This enables fast and efficient computation, and forms the core philosophy of <span class="math inline">\(\texttt{minicons}\)</span>.</p>
<pre class="python"><code>stimuli_dl = DataLoader(stimuli, batch_size = 100)</code></pre>
<p>Let’s now compute :robot:‘s scores on the 1000 sentence pairs in our evaluation stimuli.
This is done by invoking the <code>.score()</code> method of our instantiated model.
By default, it computes the log probabilities of the batch of input sentences, divided by their length (using <code>torch.mean()</code>), but also allows you to specify other aggregation functions, such as sum, geometric mean, or any other custom function that accepts a tensor of log-probabilities and returns a scalar for every row. To keep things simple, let’s call our outputs ’scores’ <span class="math inline">\((s)\)</span>.</p>
<pre class="python"><code>results = []
for batch in stimuli_dl:
    good, bad = batch
    good_score = model.score(good)
    bad_score = model.score(bad)
    results.extend(zip(good_score, bad_score))</code></pre>
<p>This gives us a results list containing <code>(good_score, bad_score)</code> tuples, each of which contain scores of the good vs. the bad inputs.
It’s typical to synthesize the results of such an experiment using an aggregated metric, to give us a summarized view of the model’s performance on the task.
For instance, one could compute the accuracy <span class="math inline">\(\mathcal{A}\)</span>, by comparing the mean log probabilities, or scores, of “good” and “bad” instances in the evaluation dataset:</p>
<p><span class="math display" id="eq:accuracy">\[
\begin{align}
\mathcal{A} = \frac{1}{n}\sum_{i = 1}^n\mathbb{1}\{s(\text{good})~&gt;~s(\text{bad})\}_i, \tag{1}
\end{align}
\]</span></p>
<p>where <span class="math inline">\(\mathbb{1}\{\}_i\)</span> is an indicator function that accepts a condition and outputs 1 if the condition is met for the given instance (<span class="math inline">\(i\)</span>), and 0 otherwise.
This is straightforward to implement in python:</p>
<pre class="python"><code>def accuracy(data):
    return statistics.mean([g &gt; b for g, b in data])
    

# Computing accuracy of our model:
print(f&quot;The accuracy of the model is {round(accuracy(results)*100, 2)}%&quot;)</code></pre>
<pre><code>## The accuracy of the model is 81.8%</code></pre>
<p>How does this compare to other models, or even humans? Let’s take a look at the paper’s results:</p>
<table>
<thead>
<tr class="header">
<th align="center">5-gram</th>
<th align="center">LSTM</th>
<th align="center">TXL</th>
<th align="center">GPT2</th>
<th align="center">Human</th>
<th align="center">:robot: (distilgpt2)</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="center">24.00%</td>
<td align="center">76.00%</td>
<td align="center">77.00%</td>
<td align="center">83.00%</td>
<td align="center">81.00%</td>
<td align="center">81.80%</td>
</tr>
</tbody>
</table>
<p>Surprisingly, our model :robot: ends up performing <em>better</em> than humans, and almost similar to GPT2 (which incidentally :robot: is actually based on)!
The observation that the model is able to <em>out-perform</em> humans should be taken with extreme caution.
One potential reason could be purely statistical — the model is optimized to predict the statistical properties of word sequences, and it could have performed slightly better than humans if it has seen certain contiguous sequences (perhaps n-gram patterns of certain words) in its training corpus. If this is true then the model should have near-perfect memory of sequence patterns. Another potential reason could be the noise in human judgment – humans aren’t asked to annotate every single pair in the dataset. In fact, the authors only selected 100 sentences from each of the 67 different phenomena for their crowdsourcing experiment.
Overall, the <a href="https://github.com/alexwarstadt/blimp">original paper</a> <span class="citation">(Warstadt et al. 2020)</span> also reports a greater differential between humans and models on more challenging grammar tasks, favoring the humans. This suggests that our language models can acquire basic facts about the English language, but still have a long way to go in order to perfectly represent a diverse set of linguistic phenomena.
For a detailed discussion on models vs. humans on these task, I urge you to read the original paper.</p>
</div>
<div id="what-else-can-be-done-using-textttminicons" class="section level2">
<h2>What else can be done using <span class="math inline">\(\texttt{minicons}\)</span>?</h2>
<p>I have only demonstrated one major application of <span class="math inline">\(\texttt{minicons}\)</span> — the ability to rapidly run relative acceptability judgments on vast amount of linguistic data.
The library also supports a whole host of other operations.
Some include:</p>
<ul>
<li>evaluating on masked language models</li>
<li>extracting hidden state representation at any layer of a given model</li>
<li>extracting and comparing the output distribution of the model on a given prompt</li>
<li>“priming” of language models where the model is conditioned on a given text and is evaluated on multiple continuations. A primitive version of this was used in my paper <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.415"><span class="citation">(Misra, Ettinger, and Rayz 2020)</span></a></li>
<li>exploring the outputs of models pre-trained to perform specific NLP tasks such as natural language inference, etc</li>
</ul>
<p>I also plan to add support for many more evaluation techniques as I make progress in my research, but I invite anyone to try and contribute to this library, especially since this is my very first time! Some potential ways of doing so could be:</p>
<ul>
<li>writing better documentation, perhaps making a website to host it all</li>
<li>writing tutorials to showcase the usefulness of the library</li>
<li>adding new functionality</li>
</ul>
<p>So feel free to create PRs/Issues and I will be happy to collaborate on them with you!</p>
</div>
<div id="closing-thoughts" class="section level2">
<h2>Closing thoughts</h2>
<p>In this post I demonstrated one crucial application of my latest python library: <span class="math inline">\(\texttt{minicons}\)</span>, by applying it on a toy evaluation setting based on basic English grammar, for a semi-fictitious model called :robot:.
Though simple, the computations supported by the library facilitate running of large-scale behavioral analyses of transformer-based language models through simple interaction with huggingface’s <code>transformer</code> library and <code>pytorch</code>.</p>
<p>The source code for this post (in RMarkdown, with python code supported through <code>reticulate</code>) can be found <a href="https://github.com/kanishkamisra/website/blob/master/content/post/2021-03-21-minicons-running-large-scale-behavioral-analyses-on-transformer-lms/index.Rmd">here</a>.</p>
<!-- Equation \@ref(eq:accuracy). -->
</div>
<div id="acknowledgments" class="section level2">
<h2>Acknowledgments</h2>
<p><em>Thanks to Hemanth Devarapalli for inspiring me to convert my code into a library and <a href="https://www.anansheth.com">Ananya Sheth</a> for suggesting fine-grained edits to this post.</em></p>
</div>
<div id="references" class="section level2 unnumbered">
<h2>References</h2>
<div id="refs" class="references">
<div id="ref-linzen2016assessing">
<p>Linzen, Tal, Emmanuel Dupoux, and Yoav Goldberg. 2016. “Assessing the Ability of Lstms to Learn Syntax-Sensitive Dependencies.” <em>Transactions of the Association for Computational Linguistics</em> 4: 521–35.</p>
</div>
<div id="ref-misra2020exploring">
<p>Misra, Kanishka, Allyson Ettinger, and Julia Rayz. 2020. “Exploring BERT’s Sensitivity to Lexical Cues Using Tests from Semantic Priming.” In <em>Findings of the Association for Computational Linguistics: EMNLP 2020</em>, 4625–35. Online: Association for Computational Linguistics. <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.415">https://www.aclweb.org/anthology/2020.findings-emnlp.415</a>.</p>
</div>
<div id="ref-vaswani2017attention">
<p>Vaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. 2017. “Attention is All you Need.” In <em>NeurIPS 2017</em>, 5998–6008.</p>
</div>
<div id="ref-warstadt2019blimp">
<p>Warstadt, Alex, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman. 2020. “BLiMP: The Benchmark of Linguistic Minimal Pairs for English.” <em>Transactions of the Association for Computational Linguistics</em> 8: 377–92. <a href="https://doi.org/10.1162/tacl\_a\_00321">https://doi.org/10.1162/tacl\_a\_00321</a>.</p>
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>the study of the internal structure of words and how they combine to form a sentence or a phrase.<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
